<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>tetratrack.videoprocessing API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tetratrack.videoprocessing</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python">import pickle
import copy

from . import utils
from .videotypes import *
from .yolo.detect import Yolo
from munkres import Munkres
from tqdm import tqdm
from typing import Optional


class StreamVideo:
    &#34;&#34;&#34;StreamVideo, Class to play video frame-by-frame

    Params:
        caption (str): Window caption
        video (str): Video filename
        w (int): Width
        h (int): Height
        start_frame (int): Start video from this frame number
        fps (int): Set video framerate (0: realtime)
        pbar (int): Progress bar id
        background (bool): Processing in background
        save_video (str): Filename to save video
        calibration_camera (np.ndarray): Matrix calibration camera
        undistort (bool): Apply frame rectification
    &#34;&#34;&#34;

    def __init__(self, caption, video, w, h, start_frame=0, fps=0, pbar=0, background=True, save_video=None, calibration_camera=None, undistort=False):
        self.h, self.w = h, w
        self.frame_number = 0
        self.background = background
        self.save_video = save_video
        self.caption = caption
        self.start_frame = start_frame
        self.calibration_camera = calibration_camera
        self.video = video
        self.undistort = undistort
        self.pbar = pbar
        self.videoCap = cv2.VideoCapture(self.video)
        self.videoCap.set(1, self.start_frame)
        self.fsampling = round(self.getFps() / fps) if fps != 0 else 1

        if self.save_video is not None:
            self.vid_writer1 = cv2.VideoWriter(self.save_video, cv2.VideoWriter_fourcc(&#39;M&#39;, &#39;J&#39;, &#39;P&#39;, &#39;G&#39;), 30,
                                          (self.w, self.h))

    def getFps(self):
        # type: () -&gt; int
        &#34;&#34;&#34;Get video-file frames per second.

        Returns:
            int: Fps
        &#34;&#34;&#34;

        return int(self.videoCap.get(cv2.CAP_PROP_FPS))

    def getVideoLength(self):
        # type: () -&gt; int
        &#34;&#34;&#34;Get total number of frames in video-file.

        Returns:
            int: Number of frames
        &#34;&#34;&#34;

        return int(self.videoCap.get(cv2.CAP_PROP_FRAME_COUNT))

    def getNextFrame(self):
        # type: () -&gt; (int, np.ndarray)
        &#34;&#34;&#34;Read next frame from video stream

        Returns:
            int: Return value
            np.ndarray: Frame
        &#34;&#34;&#34;

        ret, img = None, None
        fsample = self.fsampling

        # if no video opened return None
        if self.videoCap.isOpened():
            while fsample &gt; 0:
                ret, img = self.videoCap.read()
                self.frame_number += 1
                fsample -=1
        else:
            print(&#34;{} Error VideoCap not opened, quitting ... &#34;.format(self.video))

        return ret, img

    def processSingleFrame(self, frame, func):
        # type: (np.ndarray, classmethod) -&gt; np.ndarray
        &#34;&#34;&#34;Processing single frame

        Args:
            frame (np.ndarray): Video frame
            func (classmethod): Function to process frame

        Returns:
             np.ndarray: Frame processed
        &#34;&#34;&#34;

        # Resize frame
        frame = cv2.resize(frame, (self.w, self.h), interpolation=cv2.INTER_AREA)

        # Undistort
        if self.undistort == True:
            _, mtx, dist, _, _, newMtx = self.calibration_camera
            self.undistort = cv2.initUndistortRectifyMap(mtx, dist, None, newMtx,
                                                             (self.w, self.h), cv2.CV_32FC1)

        if isinstance(self.undistort, tuple):
            frame = cv2.remap(frame, self.undistort[0], self.undistort[1],
                                   interpolation=cv2.INTER_LINEAR, borderValue=0)


        # Call function passed as argument
        frame = func(frame, self.frame_number, self.background)

        if self.save_video is not None:
            self.vid_writer1.write(frame)

        return frame


    def play(self, func):
        # type: (classmethod) -&gt; None
        &#34;&#34;&#34;Play video frame by frame and apply a function `func`

        Args:
            func (classmethod): Generic function to execute at each frame
        &#34;&#34;&#34;

        print(&#34;Loading video...&#34;)
        ret, frame = self.getNextFrame()

        with tqdm(desc=&#39;Processing: &#39; + self.caption, position=self.pbar, unit=&#39;it&#39;, total=self.getVideoLength()) as pbar:
            while ret:
                frame = self.processSingleFrame(frame, func)
                if not self.background:
                    cv2.imshow(self.caption, frame)
                    cv2.waitKey(1)

                ret, frame = self.getNextFrame()
                pbar.update(self.fsampling)

        if self.save_video is not None:
            self.vid_writer1.release()

        print(self.caption, &#34; done!&#34;)
        return


class PeopleDetector:
    &#34;&#34;&#34;PeopleDetector, Detect human in a frame video

    Params:
        caption (str): Caption video
        videofile (str): Video filename
        startframe (int): Set initial frame number
        calibration (str): Filepath containing camera calibration values
        detector_cfg (str): Filepath containing detector architecture
        detector_weights (str): Filepath containing detector pretrained weights
    &#34;&#34;&#34;

    def __init__(self, caption, videofile, startframe, calibration, detector_cfg, detector_weights, confidence):
        self.caption = caption
        self.video = videofile
        self.start_frame = startframe
        self.aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_7X7_50)
        self.detections = []
        self.colors = {&#39;2&#39;: (255,0,0)}
        self.detection_dnn = Yolo(detector_cfg, detector_weights, confidence=confidence)
        with open(calibration, &#39;rb&#39;) as f:
            self.calibration_camera = pickle.load(f)

    def run(self, w, h, fps=0, pbar=0, background=True, undistort=False):
        # type: (int, int, int, int, bool, bool) -&gt; None
        &#34;&#34;&#34;Run Detector

        Args:
            w (int): Frame width
            h (int): Frame height
            fps (int): Framerate detections (0: realtime)
            background (bool): Process in background mode
            undistort (bool): Apply frame rectification
        &#34;&#34;&#34;

        video = StreamVideo(self.caption, self.video, w, h, self.start_frame, fps=fps, background=background, pbar=pbar, calibration_camera=self.calibration_camera, undistort=undistort)
        self.detections = [[] for _ in range(video.getVideoLength() +1)]
        video.play(self._detect)

    def _detect(self, frame, frame_number, background=False):
        # type: (np.ndarray, int, bool) -&gt; np.ndarray
        &#34;&#34;&#34;Use Yolo and Aruco detectors to extract BBox data

        Args:
            frame (np.ndarray): Frame of video
            frame_number (int): Current frame number
            background (bool): Process in background mode
        &#34;&#34;&#34;

        aruco_detections = []
        box_assignment = BoxAssignment(90, 30)

        if self.detection_dnn:

            # Detect aruco
            corners, aruco_ids, _ = cv2.aruco.detectMarkers(frame, self.aruco_dict)
            cv2.aruco.drawDetectedMarkers(frame, corners, aruco_ids, (255, 0, 0))

            # Detect people
            boxes = self.detection_dnn.predict([frame])
            boxes = boxes[0]

            # Draw box on people detected
            if not background:
                for box in boxes:
                        frame = box.draw(frame)

            if aruco_ids is None: aruco_ids = []
            iterator = ((c, a[0]) for (c, a) in zip(corners, aruco_ids)
                        if aruco_ids is not None and a[0] not in (0,4,5,6))

            for corner, aruco_id in iterator:
                box = Rect.from_corners(corner, self.calibration_camera)

                # Assign color
                if str(aruco_id) in self.colors.keys():
                    color = self.colors[str(aruco_id)]
                else:
                    color = box.random_color()
                    self.colors[str(aruco_id)] = color

                aruco_detections.append(BoundBox(aruco_id, box, color, None))

            # Aruco assignment
            if aruco_detections:
                boxes = box_assignment.assign_aruco(boxes, aruco_detections)

            for box in boxes:
                self.detections[frame_number].append(box)

        return frame

    def save_data(self, filepath):
        # type: (str) -&gt; None
        &#34;&#34;&#34;Save data extracted from video into file

        Args:
            filepath (str): Filename to save data
        &#34;&#34;&#34;

        with open(filepath, &#39;wb&#39;) as f:
            pickle.dump([self.detections], f)
        return

    def callback(self):
        # type: () -&gt; bool
        &#34;&#34;&#34;Just a useless callback for multi-thread purpose

        &#34;&#34;&#34;
        print(self.caption, &#34; acquisition done.&#34;)
        return True


class PostProcessing:
    &#34;&#34;&#34;PostProcessing, Process data extracted from video

      Params:
          videofile (str): Video filename
          startframe (int): Set initial frame number
          calibration (str): Filepath of the camera calibration file
      &#34;&#34;&#34;

    def __init__(self, videofile, startframe, calibration):
        self.video = videofile
        self.start_frame = startframe
        self.boxes_3d = None
        self.homography_data = None
        self.bird_view = None
        self.calibration_camera = None
        with open(calibration, &#39;rb&#39;) as f:
            self.calibration_camera = pickle.load(f)

    def load_data(self, filepath):
        # type: (str) -&gt; None
        &#34;&#34;&#34;Load data form file

        Args:
            filepath(str): filename contains data acquired by Detector
        &#34;&#34;&#34;

        with open(filepath, &#39;rb&#39;) as f:
            self.boxes_3d = pickle.load(f)[0]

        return None

    def save_data(self, filepath):
        # type: (str) -&gt; None
        &#34;&#34;&#34;Save data into file
        Data use this structure:
        frame1: [Box1, Box2, ...]
        frame2: [Box1, Box2, ...]
        ...
        frameN: [Box1, Box2, ...]


        Args:
            filepath (str): Filename to save data
        &#34;&#34;&#34;

        with open(filepath, &#39;wb&#39;) as f:
            pickle.dump([self.boxes_3d], f)

        return None

    def iterate_data(self):
        # type: () -&gt; (int, BoundBox)
        &#34;&#34;&#34;Generator to iterate data (BBox in each frame)

        Yields:
            int: Frame number
            BoundBox: Bounding Box
        &#34;&#34;&#34;

        for i in range(0, len(self.boxes_3d)):
            for box in self.boxes_3d[i]:
                yield i, box

    def get_video_and_homography(self, w, h, w2, h2, undistort=False):
        # type: (int, int, int, int, bool) -&gt; None
        &#34;&#34;&#34; Show tracking and homography simultaneously

        Args:
            w (int): tracking frame width
            h (int):  tracking Frame height
            w2 (int): homography frame width
            h2 (int): homography frame height
            undistort (bool): Apply rectification before processing frame
        &#34;&#34;&#34;

        video_tracking = StreamVideo(&#34;tracking&#34;, self.video, w, h, self.start_frame, calibration_camera=self.calibration_camera, undistort=undistort)
        video_homography = StreamVideo(&#34;homography&#34;, self.video, w2, h2, self.start_frame, calibration_camera=self.calibration_camera, undistort=undistort)

        retA, frameA = video_tracking.getNextFrame()
        retB, frameB = video_homography.getNextFrame()

        while retA:
            frameA = video_tracking.processSingleFrame(frameA, self._draw_boxes)
            frameB = video_homography.processSingleFrame(frameB, self._homography)

            cv2.imshow(&#34;tracking&#34;, frameA)
            cv2.waitKey(1)
            cv2.imshow(&#34;homography&#34;, frameB)
            cv2.waitKey(1)

            retA, frameA = video_tracking.getNextFrame()
            retB, frameB = video_homography.getNextFrame()

        return

    def get_homography(self, caption, w, h, background=False, save_video=None, undistort=False):
        # type: (str, int, int, bool, Optional[str], bool) -&gt; None
        &#34;&#34;&#34; Run homography video

        Args:
            caption (str): Window Name
            w (int): Frame width
            h (int): Frame height
            background (bool): Process in background mode
            save_video (str): Filename to save video
            undistort (bool): Apply rectification before processing frame
        &#34;&#34;&#34;

        video = StreamVideo(caption, self.video, w, h, self.start_frame, background, save_video, self.calibration_camera, undistort=undistort)
        video.play(self._homography)

    def compute_homography(self, pts_src, pts_dst, pic):
        # type: (np.ndarray, np.ndarray, str) -&gt; None
        &#34;&#34;&#34;Compute homography

        Args:
            pts_src (np.ndarray): Source points to compute homography
            pts_dst (np.ndarray): Destination points to compute homography
            pic (np.ndarray): Filename of a birdview picture
        &#34;&#34;&#34;

        pic = cv2.imread(pic, 0)
        self.bird_view = cv2.cvtColor(pic, cv2.COLOR_GRAY2BGR)

        pts_src = np.expand_dims(pts_src, 1).astype(np.float32)
        pts_dst = np.expand_dims(pts_dst, 1).astype(np.float32)
        hom = cv2.getPerspectiveTransform(pts_src, pts_dst)

        #pic2 = cv2.warpPerspective(pic, hom, (430, 813))
        #cv2.imshow(&#39;image&#39;, pic2)
        #cv2.waitKey(0)

        self.homography_data = copy.deepcopy(self.boxes_3d)
        iterator = ((i, j, box) for i in range(len(self.homography_data))
                    for j, box in enumerate(self.homography_data[i]))

        for i, j, box in iterator:
            x, y = box.rect.c_x, (box.rect.y + box.rect.h)
            points = np.array([[[x, y]]], dtype=&#39;float32&#39;)
            points = cv2.perspectiveTransform(points, hom)
            self.homography_data[i][j] = PointId(box.id, points[0][0][0], points[0][0][1], box.color)

    def _homography(self, frame, current_frame, _):
        # type: (np.ndarray, int, np.ndarray) -&gt; np.ndarray
        &#34;&#34;&#34; Processing frame function

        Args:
            frame (np.ndarray): Video frame
            current_frame (int): Number of the current frame

        Returns:
            np.ndarray: Frame with Points drawn on it
        &#34;&#34;&#34;

        bird_view = copy.deepcopy(self.bird_view)
        for point in self.homography_data[current_frame]:
            point.draw(bird_view)
        return bird_view

    def _smoot_get(self):
        # type: () -&gt; dict
        &#34;&#34;&#34;Auxiliary function to apply smoothing

        Returns:
            dict: Dict with BBox points on which to apply smooth
        &#34;&#34;&#34;

        h_points = {}
        iterator = self.iterate_data()

        for _, box in iterator:
            if box.id not in h_points.keys():
                h_points[box.id] = []
            h_points[box.id].append((box.rect.x, box.rect.y, box.rect.h, box.rect.w))
        return h_points

    def smooth(self, kernel_size):
        # type: (int) -&gt; None
        &#34;&#34;&#34;Smooth BBox

        Args:
            kernel_size (int): Number specifying kernel size (high more smoothing)

        &gt;&gt; NOTE: Smoothing is applied only to (y, h, w) BBox params
                 Smoothing on X coordinate is NOT visually good
        &#34;&#34;&#34;

        h_points = self._smoot_get()
        for key, val in h_points.items():
            h_points[key] = utils.smooth(np.array(val), kernel_size, kernel_dim=4)

        h_indexes = {}
        iterator = self.iterate_data()

        for _, box in iterator:
            if box.id not in h_indexes:
                h_indexes[box.id] = 0
            ix = h_indexes[box.id]
            box.rect.y = int(h_points[box.id][ix][1])
            box.rect.h = int(h_points[box.id][ix][2])
            box.rect.w = int(h_points[box.id][ix][3])
            h_indexes[box.id] += 1

    def _predict_z(self):
        # type: () -&gt; (float, float)
        &#34;&#34;&#34;Regression BBox points (y, depth)

        &gt;&gt;&gt; NOTE use y the bottom point of Bounding Box instead of Height

        Returns:
            (float, float): linear function coefficients (m, q)
        &#34;&#34;&#34;

        box_h, box_z = [], []
        iterator = self.iterate_data()

        for _, box in iterator:
            if box.rect.z is not None:
                box_h.append(box.rect.y + box.rect.h)
                box_z.append(box.rect.z)

        coeff = np.polyfit(np.array(box_h), np.array(box_z), 1)
        return coeff[0], coeff[1]

    def fill_z(self):
        # type: () -&gt; None
        &#34;&#34;&#34;Estimate missing depth information of BBox in data
           and fill them inplace

        &#34;&#34;&#34;

        m, q = self._predict_z()
        iterator = self.iterate_data()

        for _, box in iterator:
            if box.rect.z is None:
                box.rect.z = (box.rect.y + box.rect.h) * m + q

    def assign_id(self, mode=&#39;3d-camera&#39;, threshold=120, max_age=30, aruco_priority=True):
        # type: (str, int, int, bool) -&gt; None
        &#34;&#34;&#34;Assign BBox ids

        Args:
            mode (str): Distance mode: (3d, bottom, center)
            threshold (int): Tolerance threshold
            max_age (int): Max number of frames than remove BBox
            aruco_priority (bool): Assign aruco ID if available
        &#34;&#34;&#34;

        first_frame = 0
        box_assignment = BoxAssignment(threshold, max_age, self.calibration_camera)

        while len(self.boxes_3d[first_frame]) &lt; 1:
            first_frame += 1

        box_assignment.init(self.boxes_3d[first_frame])
        for i in range(first_frame, len(self.boxes_3d)):
            self.boxes_3d = box_assignment.assign(self.boxes_3d, i, mode, aruco_priority)

    def get_video(self, caption, w, h, background=False, save_video=None, undistort=False):
        # type: (str, int, int, bool, Optional[str], bool) -&gt; None
        &#34;&#34;&#34;Execute human detection

        Args:
            caption (str): Window Name
            w (int): Frame width
            h (int): Frame height
            background (bool): Process in background mode
            save_video (str): Filename to save video
            undistort (bool): Apply rectification before processing frame
        &#34;&#34;&#34;

        video = StreamVideo(caption, self.video, w, h, start_frame=self.start_frame, background=background, save_video=save_video, calibration_camera=self.calibration_camera, undistort=undistort)
        video.play(self._draw_boxes)

    def _draw_boxes(self, frame, frame_number, _):
        # type: (np.ndarray, int) -&gt; np.ndarray
        &#34;&#34;&#34;

        Args:
            frame (np.ndarray): Frame of video
            frame_number (int): Current frame number

        Returns:
            np.ndarray: Frame with BBox drawn on it
        &#34;&#34;&#34;

        for box in self.boxes_3d[frame_number]:
            frame = box.draw(frame)
        return frame

    def _prec(self, id, frame_num, window):
        # type: (int, int, int) -&gt; BoundBox or None
        &#34;&#34;&#34;Find previous BBox with id = id

        Args:
            id (int): BBox ID
            frame_num (int): Current frame number
            window (int): Temporal frames window to search
        &#34;&#34;&#34;

        window = frame_num - window
        while frame_num &gt; window and frame_num &gt; 0:
            iterator = (box for box in self.boxes_3d[frame_num] if (box.id == id) and (box.age == 0))
            for box in iterator:
                return box
            frame_num -= 1
        return None

    def _succ(self, id, frame_num, window):
        # type: (int, int, int) -&gt; BoundBox or None
        &#34;&#34;&#34;Find next BBox with id = id

        Args:
            id (int): BBox ID
            frame_num (int): Current frame number
            window (int): Temporal frames window to search
        &#34;&#34;&#34;

        window = frame_num + window
        while frame_num &lt; window and frame_num &lt; len(self.boxes_3d):
            iterator = (box for box in self.boxes_3d[frame_num] if (box.id == id) and (box.age == 0))
            for box in iterator:
                return box
            frame_num += 1
        return None

    def __get_ids_at_frame(self, frame_num):
        # type: (int) -&gt; list
        &#34;&#34;&#34;Get all BBox ids at frame = frame_num

        Args:
            frame_num (int): Frame number

        Returns:
            list: List of BBox ids
        &#34;&#34;&#34;

        box_in_frame = []
        for box in self.boxes_3d[frame_num]:
            box_in_frame.append(box.id)
        return box_in_frame

    def __get_all_id(self):
        # type: () -&gt; list
        &#34;&#34;&#34;Get all uniques BBox ids in data

        Returns:
              list: List of ids
        &#34;&#34;&#34;

        box_ids = []
        iterator = self.iterate_data()
        iterator = ((_, box) for _, box in iterator if box.id not in box_ids)

        for _, box in iterator:
            box_ids.append(box.id)
        return box_ids

    def fill(self, window=90):
        # type: (int) -&gt; None
        &#34;&#34;&#34;Fill missing detections interpolating data

        Args:
            window (int): Temporal window size
        &#34;&#34;&#34;

        all_ids = self.__get_all_id()
        iterator = ((i, _id) for _id in all_ids for i in range(0, len(self.boxes_3d))
                    if _id not in self.__get_ids_at_frame(i))

        for i, _id in iterator:
            prec_box, next_box = self._prec(_id, i, window), self._succ(_id, i, window)
            if prec_box is not None and next_box is not None:
                prec_box.rect = prec_box.rect.interpolate(next_box.rect)
                self.boxes_3d[i].append(prec_box)
        return None

    def plot_3d(self, box_id, savepath=None):
        # type: (int, str) -&gt; None
        &#34;&#34;&#34;Plot 3d path of a BBox with id = box_id

        Args:
            box_id (int): ID BBox to plot
            savepath (str): Filename to save video
        &#34;&#34;&#34;

        points = []
        out = None
        iterator = self.iterate_data()

        if savepath is not None:
            out = cv2.VideoWriter(savepath, cv2.VideoWriter_fourcc(&#39;M&#39;, &#39;J&#39;, &#39;P&#39;, &#39;G&#39;), 30, (800, 800))

        for i, box in iterator:
            if box.id == box_id:
                points.append((box.rect.x, box.rect.y+box.rect.h, box.rect.z))
            else:
                points.append((None, None, None))

        ts = np.array(points, dtype=np.float32)
        for i in range(1, len(ts)):
            fig = utils.show_temporal_sequence(ts[:i], center=np.nanmean(ts, 0), radius=np.sqrt(np.nanvar(ts)),
                                               colormap=&#39;jet&#39;, swapaxis=False)
            fig.savefig(&#34;frame.png&#34;)
            img = cv2.imread(&#34;frame.png&#34;)

            if out:
                out.write(img)
            else:
                cv2.imshow(&#34;Plot 3d&#34;, img)
                cv2.waitKey(1)
        if out:
            out.release()
        return


class BoxAssignment:
    &#34;&#34;&#34;BoxAssignment, BBox ID re-assignment

    Params:
        threshold (int): Threshold tolerance
        max_age (int): Max number of frame, than remove BBox
        calibration_camera (np.ndarray): Matrix calibration camera
    &#34;&#34;&#34;

    def __init__(self, threshold, max_age, calibration_camera=None):
        self.threshold = threshold
        self.max_age = max_age
        self.aruco_dict = {}
        self.aruco_ids = []
        self.boxes = []
        self.n_boxes = 0
        self.highest_id = -1
        self.calibration_camera = calibration_camera

    def init(self, init_boxes):
        # type: (list) -&gt; None
        &#34;&#34;&#34;Initialize BBox assignment

        Args:
            init_boxes (list): List of Bounding Box
        &#34;&#34;&#34;

        [self._add_new_box(box) for box in init_boxes if not self.boxes]

    def assign_aruco(self, boxes, corners):
        # type: (list, list) -&gt; list
        &#34;&#34;&#34;Assign aruco id to the nearest BBox

        Args:
            boxes (list): List of BBox
            corners (list): List of Aruco-Box

        Returns:
            list: List of BBox with the aruco id

        &#34;&#34;&#34;

        cost_matrix = self._cost_matrix(corners, boxes, mode=&#39;center&#39;)
        m = Munkres()
        indexes = m.compute(cost_matrix)

        for idx in indexes:
            aruco_id, box_id = idx[0], idx[1]
            boxes[box_id].id = corners[aruco_id].id
            boxes[box_id].color = corners[aruco_id].color
            boxes[box_id].rect.z = corners[aruco_id].rect.z

        return boxes

    def assign(self, boxes_3d, frame_num, mode, aruco_priority=True):
        # type: (list, int, str, bool) -&gt; list
        &#34;&#34;&#34;Bounding box Re-identification.
           Use same id of the previous frame for pre-existent BBox
           Use new id for new-entry BBox

        Args:
            boxes_3d (list): Data extracted from video (List of BBox for each frame)
            frame_num (int): Current frame number
            mode (str): Assignment mode (3d, bottom, center)
            aruco_priority (bool): If available use aruco id

        Returns:
            list: List of Bounding box with id assigned
        &#34;&#34;&#34;

        boxes = boxes_3d[frame_num]
        current_box = []
        found_new_box = True

        # Check box in this frame
        if not boxes:
            return boxes_3d

        # Aging box
        self._age_box_update()

        # Delete old box
        [self.boxes.remove(box) for box in self.boxes if box.age &gt; self.max_age]

        while found_new_box:
            # Compute cost matrix
            cost_matrix = self._cost_matrix(boxes, self.boxes, mode=mode)

            # Hungarian algorithm
            m = Munkres()
            indexes = m.compute(cost_matrix)
            found_new_box = False

            #  Assign ID
            for idx, box, cost in zip(indexes, boxes, cost_matrix):
                id_box, id_saved_box = idx[0], idx[1]

                # Assign ID if cost &lt; threshold
                if cost[id_saved_box] &lt; self.threshold:

                    # If available use aruco id
                    if aruco_priority and box.id is not None:
                        if box.id != self.boxes[id_saved_box].id and self.boxes[id_saved_box].id not in self.aruco_ids:
                            self.aruco_ids.append(box.id)
                            boxes_3d = self._replace_id(boxes_3d, frame_num, self.boxes[id_saved_box].id, box.id,
                                                        box.color)
                            self.boxes[id_saved_box].id = box.id
                            self.boxes[id_saved_box].color = box.color

                    self.boxes[id_saved_box].rect = box.rect
                    self.boxes[id_saved_box].age = 0

                # Add new box if cost &gt; threshold
                else:
                    self._add_new_box(box)
                    found_new_box = True

        # Update box list in the current frame
        [current_box.append(box) for box in self.boxes if box.age == 0]
        boxes_3d[frame_num] = copy.deepcopy(current_box)
        return boxes_3d

    @staticmethod
    def _replace_id(boxes_3d, frame_num, id, new_id, color):
        # type: (list, int, int, int, (int, int, int)) -&gt; list
        &#34;&#34;&#34;Replace BBox id with new_id for each frame &lt; frame_num

        Args:
            boxes_3d (list): Data extracted from video (List of BBox for each frame)
            frame_num (int): Frame number
            id (int): ID to replace
            new_id (int): New ID
            color (int): Color to use

        Returns:
            list : List of Bounding Box with replaced ids
        &#34;&#34;&#34;

        iterator = (box for i in range(0, frame_num) for box in boxes_3d[i] if box.id == id)
        for box in iterator:
            box.id = new_id
            box.color = color
        return boxes_3d

    def _cost_matrix(self, rows, cols, mode=&#39;bottom&#39;):
        # type: (list, list, str) -&gt; list
        &#34;&#34;&#34;Compute cost matrix

        Args:
            rows (list): List of BBox
            cols (list): List of BBox
            mode (str): Distance mode (3d, bottom, center)

        Returns:
            list: Cost Matrix
        &#34;&#34;&#34;

        cost_matrix = []

        for r in rows:
            row = []
            for c in cols:
                a, b = None, None
                if mode == &#39;center&#39;:
                    a = (r.rect.c_x, r.rect.c_y)
                    b = (c.rect.c_x, c.rect.c_y)
                elif mode == &#39;bottom&#39;:
                    a = (r.rect.c_x, r.rect.y + r.rect.h)
                    b = (c.rect.c_x, c.rect.y + c.rect.h)
                elif mode == &#39;3d-camera&#39;:
                    a = (r.rect.c_x, r.rect.y + r.rect.h, r.rect.z)
                    b = (c.rect.c_x, c.rect.y + c.rect.h, c.rect.z)
                elif mode == &#39;3d-world&#39;:
                    a = r.rect.to_world_coordinates(self.calibration_camera, scale_factor=np.array((2.4, 1.8)))
                    b = c.rect.to_world_coordinates(self.calibration_camera, scale_factor=np.array((2.4, 1.8)))
                    a = (a.c_x, a.y + a.h, a.z)
                    b = (b.c_x, b.y + b.h, b.z)

                cost = self._distance_euclidean(a, b)
                row.append(cost)
            cost_matrix.append(row)

        return cost_matrix

    def _age_box_update(self):
        # type: () -&gt; None
        &#34;&#34;&#34;Update age in BBox
        &#34;&#34;&#34;

        for box in self.boxes:
            box.age += 1

    def _get_new_id(self):
        # type: () -&gt; int
        &#34;&#34;&#34;Get max-id number assigned

        Returns:
            int: max ID
        &#34;&#34;&#34;

        self.highest_id += 1
        while self.highest_id in self.aruco_ids:
            self.highest_id += 1
        return self.highest_id

    def _add_new_box(self, box):
        # type: (BoundBox) -&gt; None
        &#34;&#34;&#34;Add BBox into the list of active BBox

        Args:
            box: BBox
        &#34;&#34;&#34;

        id, age = self._get_new_id(), 0
        color = (randint(64, 255), randint(64, 255), randint(64, 255))
        self.boxes.append(BoundBox(id, box.rect, color, age))
        self.n_boxes += 1

    @staticmethod
    def _distance_euclidean(a, b):
        # type: ((int, int), (int, int)) -&gt; float
        &#34;&#34;&#34;Compute euclidean distance

        Args:
            a: Point coordinates (2d or 3d)
            b: Point coordinates (2d or 3d)

        Returns:
            float: Distance value
        &#34;&#34;&#34;

        assert len(a) == len(b)

        val = 0
        for i in range(len(a)):
            val += (a[i] - b[i]) ** 2
        return val ** (1 / 2)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tetratrack.videoprocessing.BoxAssignment"><code class="flex name class">
<span>class <span class="ident">BoxAssignment</span></span>
<span>(</span><span>threshold, max_age, calibration_camera=None)</span>
</code></dt>
<dd>
<section class="desc"><p>BoxAssignment, BBox ID re-assignment</p>
<h2 id="params">Params</h2>
<dl>
<dt><strong><code>threshold</code></strong> :&ensp;<code>int</code></dt>
<dd>Threshold tolerance</dd>
<dt><strong><code>max_age</code></strong> :&ensp;<code>int</code></dt>
<dd>Max number of frame, than remove BBox</dd>
<dt><strong><code>calibration_camera</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Matrix calibration camera</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class BoxAssignment:
    &#34;&#34;&#34;BoxAssignment, BBox ID re-assignment

    Params:
        threshold (int): Threshold tolerance
        max_age (int): Max number of frame, than remove BBox
        calibration_camera (np.ndarray): Matrix calibration camera
    &#34;&#34;&#34;

    def __init__(self, threshold, max_age, calibration_camera=None):
        self.threshold = threshold
        self.max_age = max_age
        self.aruco_dict = {}
        self.aruco_ids = []
        self.boxes = []
        self.n_boxes = 0
        self.highest_id = -1
        self.calibration_camera = calibration_camera

    def init(self, init_boxes):
        # type: (list) -&gt; None
        &#34;&#34;&#34;Initialize BBox assignment

        Args:
            init_boxes (list): List of Bounding Box
        &#34;&#34;&#34;

        [self._add_new_box(box) for box in init_boxes if not self.boxes]

    def assign_aruco(self, boxes, corners):
        # type: (list, list) -&gt; list
        &#34;&#34;&#34;Assign aruco id to the nearest BBox

        Args:
            boxes (list): List of BBox
            corners (list): List of Aruco-Box

        Returns:
            list: List of BBox with the aruco id

        &#34;&#34;&#34;

        cost_matrix = self._cost_matrix(corners, boxes, mode=&#39;center&#39;)
        m = Munkres()
        indexes = m.compute(cost_matrix)

        for idx in indexes:
            aruco_id, box_id = idx[0], idx[1]
            boxes[box_id].id = corners[aruco_id].id
            boxes[box_id].color = corners[aruco_id].color
            boxes[box_id].rect.z = corners[aruco_id].rect.z

        return boxes

    def assign(self, boxes_3d, frame_num, mode, aruco_priority=True):
        # type: (list, int, str, bool) -&gt; list
        &#34;&#34;&#34;Bounding box Re-identification.
           Use same id of the previous frame for pre-existent BBox
           Use new id for new-entry BBox

        Args:
            boxes_3d (list): Data extracted from video (List of BBox for each frame)
            frame_num (int): Current frame number
            mode (str): Assignment mode (3d, bottom, center)
            aruco_priority (bool): If available use aruco id

        Returns:
            list: List of Bounding box with id assigned
        &#34;&#34;&#34;

        boxes = boxes_3d[frame_num]
        current_box = []
        found_new_box = True

        # Check box in this frame
        if not boxes:
            return boxes_3d

        # Aging box
        self._age_box_update()

        # Delete old box
        [self.boxes.remove(box) for box in self.boxes if box.age &gt; self.max_age]

        while found_new_box:
            # Compute cost matrix
            cost_matrix = self._cost_matrix(boxes, self.boxes, mode=mode)

            # Hungarian algorithm
            m = Munkres()
            indexes = m.compute(cost_matrix)
            found_new_box = False

            #  Assign ID
            for idx, box, cost in zip(indexes, boxes, cost_matrix):
                id_box, id_saved_box = idx[0], idx[1]

                # Assign ID if cost &lt; threshold
                if cost[id_saved_box] &lt; self.threshold:

                    # If available use aruco id
                    if aruco_priority and box.id is not None:
                        if box.id != self.boxes[id_saved_box].id and self.boxes[id_saved_box].id not in self.aruco_ids:
                            self.aruco_ids.append(box.id)
                            boxes_3d = self._replace_id(boxes_3d, frame_num, self.boxes[id_saved_box].id, box.id,
                                                        box.color)
                            self.boxes[id_saved_box].id = box.id
                            self.boxes[id_saved_box].color = box.color

                    self.boxes[id_saved_box].rect = box.rect
                    self.boxes[id_saved_box].age = 0

                # Add new box if cost &gt; threshold
                else:
                    self._add_new_box(box)
                    found_new_box = True

        # Update box list in the current frame
        [current_box.append(box) for box in self.boxes if box.age == 0]
        boxes_3d[frame_num] = copy.deepcopy(current_box)
        return boxes_3d

    @staticmethod
    def _replace_id(boxes_3d, frame_num, id, new_id, color):
        # type: (list, int, int, int, (int, int, int)) -&gt; list
        &#34;&#34;&#34;Replace BBox id with new_id for each frame &lt; frame_num

        Args:
            boxes_3d (list): Data extracted from video (List of BBox for each frame)
            frame_num (int): Frame number
            id (int): ID to replace
            new_id (int): New ID
            color (int): Color to use

        Returns:
            list : List of Bounding Box with replaced ids
        &#34;&#34;&#34;

        iterator = (box for i in range(0, frame_num) for box in boxes_3d[i] if box.id == id)
        for box in iterator:
            box.id = new_id
            box.color = color
        return boxes_3d

    def _cost_matrix(self, rows, cols, mode=&#39;bottom&#39;):
        # type: (list, list, str) -&gt; list
        &#34;&#34;&#34;Compute cost matrix

        Args:
            rows (list): List of BBox
            cols (list): List of BBox
            mode (str): Distance mode (3d, bottom, center)

        Returns:
            list: Cost Matrix
        &#34;&#34;&#34;

        cost_matrix = []

        for r in rows:
            row = []
            for c in cols:
                a, b = None, None
                if mode == &#39;center&#39;:
                    a = (r.rect.c_x, r.rect.c_y)
                    b = (c.rect.c_x, c.rect.c_y)
                elif mode == &#39;bottom&#39;:
                    a = (r.rect.c_x, r.rect.y + r.rect.h)
                    b = (c.rect.c_x, c.rect.y + c.rect.h)
                elif mode == &#39;3d-camera&#39;:
                    a = (r.rect.c_x, r.rect.y + r.rect.h, r.rect.z)
                    b = (c.rect.c_x, c.rect.y + c.rect.h, c.rect.z)
                elif mode == &#39;3d-world&#39;:
                    a = r.rect.to_world_coordinates(self.calibration_camera, scale_factor=np.array((2.4, 1.8)))
                    b = c.rect.to_world_coordinates(self.calibration_camera, scale_factor=np.array((2.4, 1.8)))
                    a = (a.c_x, a.y + a.h, a.z)
                    b = (b.c_x, b.y + b.h, b.z)

                cost = self._distance_euclidean(a, b)
                row.append(cost)
            cost_matrix.append(row)

        return cost_matrix

    def _age_box_update(self):
        # type: () -&gt; None
        &#34;&#34;&#34;Update age in BBox
        &#34;&#34;&#34;

        for box in self.boxes:
            box.age += 1

    def _get_new_id(self):
        # type: () -&gt; int
        &#34;&#34;&#34;Get max-id number assigned

        Returns:
            int: max ID
        &#34;&#34;&#34;

        self.highest_id += 1
        while self.highest_id in self.aruco_ids:
            self.highest_id += 1
        return self.highest_id

    def _add_new_box(self, box):
        # type: (BoundBox) -&gt; None
        &#34;&#34;&#34;Add BBox into the list of active BBox

        Args:
            box: BBox
        &#34;&#34;&#34;

        id, age = self._get_new_id(), 0
        color = (randint(64, 255), randint(64, 255), randint(64, 255))
        self.boxes.append(BoundBox(id, box.rect, color, age))
        self.n_boxes += 1

    @staticmethod
    def _distance_euclidean(a, b):
        # type: ((int, int), (int, int)) -&gt; float
        &#34;&#34;&#34;Compute euclidean distance

        Args:
            a: Point coordinates (2d or 3d)
            b: Point coordinates (2d or 3d)

        Returns:
            float: Distance value
        &#34;&#34;&#34;

        assert len(a) == len(b)

        val = 0
        for i in range(len(a)):
            val += (a[i] - b[i]) ** 2
        return val ** (1 / 2)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tetratrack.videoprocessing.BoxAssignment.assign"><code class="name flex">
<span>def <span class="ident">assign</span></span>(<span>self, boxes_3d, frame_num, mode, aruco_priority=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Bounding box Re-identification.
Use same id of the previous frame for pre-existent BBox
Use new id for new-entry BBox</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes_3d</code></strong> :&ensp;<code>list</code></dt>
<dd>Data extracted from video (List of BBox for each frame)</dd>
<dt><strong><code>frame_num</code></strong> :&ensp;<code>int</code></dt>
<dd>Current frame number</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>Assignment mode (3d, bottom, center)</dd>
<dt><strong><code>aruco_priority</code></strong> :&ensp;<code>bool</code></dt>
<dd>If available use aruco id</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>List of Bounding box with id assigned</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def assign(self, boxes_3d, frame_num, mode, aruco_priority=True):
    # type: (list, int, str, bool) -&gt; list
    &#34;&#34;&#34;Bounding box Re-identification.
       Use same id of the previous frame for pre-existent BBox
       Use new id for new-entry BBox

    Args:
        boxes_3d (list): Data extracted from video (List of BBox for each frame)
        frame_num (int): Current frame number
        mode (str): Assignment mode (3d, bottom, center)
        aruco_priority (bool): If available use aruco id

    Returns:
        list: List of Bounding box with id assigned
    &#34;&#34;&#34;

    boxes = boxes_3d[frame_num]
    current_box = []
    found_new_box = True

    # Check box in this frame
    if not boxes:
        return boxes_3d

    # Aging box
    self._age_box_update()

    # Delete old box
    [self.boxes.remove(box) for box in self.boxes if box.age &gt; self.max_age]

    while found_new_box:
        # Compute cost matrix
        cost_matrix = self._cost_matrix(boxes, self.boxes, mode=mode)

        # Hungarian algorithm
        m = Munkres()
        indexes = m.compute(cost_matrix)
        found_new_box = False

        #  Assign ID
        for idx, box, cost in zip(indexes, boxes, cost_matrix):
            id_box, id_saved_box = idx[0], idx[1]

            # Assign ID if cost &lt; threshold
            if cost[id_saved_box] &lt; self.threshold:

                # If available use aruco id
                if aruco_priority and box.id is not None:
                    if box.id != self.boxes[id_saved_box].id and self.boxes[id_saved_box].id not in self.aruco_ids:
                        self.aruco_ids.append(box.id)
                        boxes_3d = self._replace_id(boxes_3d, frame_num, self.boxes[id_saved_box].id, box.id,
                                                    box.color)
                        self.boxes[id_saved_box].id = box.id
                        self.boxes[id_saved_box].color = box.color

                self.boxes[id_saved_box].rect = box.rect
                self.boxes[id_saved_box].age = 0

            # Add new box if cost &gt; threshold
            else:
                self._add_new_box(box)
                found_new_box = True

    # Update box list in the current frame
    [current_box.append(box) for box in self.boxes if box.age == 0]
    boxes_3d[frame_num] = copy.deepcopy(current_box)
    return boxes_3d</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.BoxAssignment.assign_aruco"><code class="name flex">
<span>def <span class="ident">assign_aruco</span></span>(<span>self, boxes, corners)</span>
</code></dt>
<dd>
<section class="desc"><p>Assign aruco id to the nearest BBox</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boxes</code></strong> :&ensp;<code>list</code></dt>
<dd>List of BBox</dd>
<dt><strong><code>corners</code></strong> :&ensp;<code>list</code></dt>
<dd>List of Aruco-Box</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>list</code></strong></dt>
<dd>List of BBox with the aruco id</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def assign_aruco(self, boxes, corners):
    # type: (list, list) -&gt; list
    &#34;&#34;&#34;Assign aruco id to the nearest BBox

    Args:
        boxes (list): List of BBox
        corners (list): List of Aruco-Box

    Returns:
        list: List of BBox with the aruco id

    &#34;&#34;&#34;

    cost_matrix = self._cost_matrix(corners, boxes, mode=&#39;center&#39;)
    m = Munkres()
    indexes = m.compute(cost_matrix)

    for idx in indexes:
        aruco_id, box_id = idx[0], idx[1]
        boxes[box_id].id = corners[aruco_id].id
        boxes[box_id].color = corners[aruco_id].color
        boxes[box_id].rect.z = corners[aruco_id].rect.z

    return boxes</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.BoxAssignment.init"><code class="name flex">
<span>def <span class="ident">init</span></span>(<span>self, init_boxes)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize BBox assignment</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>init_boxes</code></strong> :&ensp;<code>list</code></dt>
<dd>List of Bounding Box</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def init(self, init_boxes):
    # type: (list) -&gt; None
    &#34;&#34;&#34;Initialize BBox assignment

    Args:
        init_boxes (list): List of Bounding Box
    &#34;&#34;&#34;

    [self._add_new_box(box) for box in init_boxes if not self.boxes]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="tetratrack.videoprocessing.PeopleDetector"><code class="flex name class">
<span>class <span class="ident">PeopleDetector</span></span>
<span>(</span><span>caption, videofile, startframe, calibration, detector_cfg, detector_weights, confidence)</span>
</code></dt>
<dd>
<section class="desc"><p>PeopleDetector, Detect human in a frame video</p>
<h2 id="params">Params</h2>
<dl>
<dt><strong><code>caption</code></strong> :&ensp;<code>str</code></dt>
<dd>Caption video</dd>
<dt><strong><code>videofile</code></strong> :&ensp;<code>str</code></dt>
<dd>Video filename</dd>
<dt><strong><code>startframe</code></strong> :&ensp;<code>int</code></dt>
<dd>Set initial frame number</dd>
<dt><strong><code>calibration</code></strong> :&ensp;<code>str</code></dt>
<dd>Filepath containing camera calibration values</dd>
<dt><strong><code>detector_cfg</code></strong> :&ensp;<code>str</code></dt>
<dd>Filepath containing detector architecture</dd>
<dt><strong><code>detector_weights</code></strong> :&ensp;<code>str</code></dt>
<dd>Filepath containing detector pretrained weights</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class PeopleDetector:
    &#34;&#34;&#34;PeopleDetector, Detect human in a frame video

    Params:
        caption (str): Caption video
        videofile (str): Video filename
        startframe (int): Set initial frame number
        calibration (str): Filepath containing camera calibration values
        detector_cfg (str): Filepath containing detector architecture
        detector_weights (str): Filepath containing detector pretrained weights
    &#34;&#34;&#34;

    def __init__(self, caption, videofile, startframe, calibration, detector_cfg, detector_weights, confidence):
        self.caption = caption
        self.video = videofile
        self.start_frame = startframe
        self.aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_7X7_50)
        self.detections = []
        self.colors = {&#39;2&#39;: (255,0,0)}
        self.detection_dnn = Yolo(detector_cfg, detector_weights, confidence=confidence)
        with open(calibration, &#39;rb&#39;) as f:
            self.calibration_camera = pickle.load(f)

    def run(self, w, h, fps=0, pbar=0, background=True, undistort=False):
        # type: (int, int, int, int, bool, bool) -&gt; None
        &#34;&#34;&#34;Run Detector

        Args:
            w (int): Frame width
            h (int): Frame height
            fps (int): Framerate detections (0: realtime)
            background (bool): Process in background mode
            undistort (bool): Apply frame rectification
        &#34;&#34;&#34;

        video = StreamVideo(self.caption, self.video, w, h, self.start_frame, fps=fps, background=background, pbar=pbar, calibration_camera=self.calibration_camera, undistort=undistort)
        self.detections = [[] for _ in range(video.getVideoLength() +1)]
        video.play(self._detect)

    def _detect(self, frame, frame_number, background=False):
        # type: (np.ndarray, int, bool) -&gt; np.ndarray
        &#34;&#34;&#34;Use Yolo and Aruco detectors to extract BBox data

        Args:
            frame (np.ndarray): Frame of video
            frame_number (int): Current frame number
            background (bool): Process in background mode
        &#34;&#34;&#34;

        aruco_detections = []
        box_assignment = BoxAssignment(90, 30)

        if self.detection_dnn:

            # Detect aruco
            corners, aruco_ids, _ = cv2.aruco.detectMarkers(frame, self.aruco_dict)
            cv2.aruco.drawDetectedMarkers(frame, corners, aruco_ids, (255, 0, 0))

            # Detect people
            boxes = self.detection_dnn.predict([frame])
            boxes = boxes[0]

            # Draw box on people detected
            if not background:
                for box in boxes:
                        frame = box.draw(frame)

            if aruco_ids is None: aruco_ids = []
            iterator = ((c, a[0]) for (c, a) in zip(corners, aruco_ids)
                        if aruco_ids is not None and a[0] not in (0,4,5,6))

            for corner, aruco_id in iterator:
                box = Rect.from_corners(corner, self.calibration_camera)

                # Assign color
                if str(aruco_id) in self.colors.keys():
                    color = self.colors[str(aruco_id)]
                else:
                    color = box.random_color()
                    self.colors[str(aruco_id)] = color

                aruco_detections.append(BoundBox(aruco_id, box, color, None))

            # Aruco assignment
            if aruco_detections:
                boxes = box_assignment.assign_aruco(boxes, aruco_detections)

            for box in boxes:
                self.detections[frame_number].append(box)

        return frame

    def save_data(self, filepath):
        # type: (str) -&gt; None
        &#34;&#34;&#34;Save data extracted from video into file

        Args:
            filepath (str): Filename to save data
        &#34;&#34;&#34;

        with open(filepath, &#39;wb&#39;) as f:
            pickle.dump([self.detections], f)
        return

    def callback(self):
        # type: () -&gt; bool
        &#34;&#34;&#34;Just a useless callback for multi-thread purpose

        &#34;&#34;&#34;
        print(self.caption, &#34; acquisition done.&#34;)
        return True</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tetratrack.videoprocessing.PeopleDetector.callback"><code class="name flex">
<span>def <span class="ident">callback</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Just a useless callback for multi-thread purpose</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def callback(self):
    # type: () -&gt; bool
    &#34;&#34;&#34;Just a useless callback for multi-thread purpose

    &#34;&#34;&#34;
    print(self.caption, &#34; acquisition done.&#34;)
    return True</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PeopleDetector.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, w, h, fps=0, pbar=0, background=True, undistort=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Run Detector</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>w</code></strong> :&ensp;<code>int</code></dt>
<dd>Frame width</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>int</code></dt>
<dd>Frame height</dd>
<dt><strong><code>fps</code></strong> :&ensp;<code>int</code></dt>
<dd>Framerate detections (0: realtime)</dd>
<dt><strong><code>background</code></strong> :&ensp;<code>bool</code></dt>
<dd>Process in background mode</dd>
<dt><strong><code>undistort</code></strong> :&ensp;<code>bool</code></dt>
<dd>Apply frame rectification</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def run(self, w, h, fps=0, pbar=0, background=True, undistort=False):
    # type: (int, int, int, int, bool, bool) -&gt; None
    &#34;&#34;&#34;Run Detector

    Args:
        w (int): Frame width
        h (int): Frame height
        fps (int): Framerate detections (0: realtime)
        background (bool): Process in background mode
        undistort (bool): Apply frame rectification
    &#34;&#34;&#34;

    video = StreamVideo(self.caption, self.video, w, h, self.start_frame, fps=fps, background=background, pbar=pbar, calibration_camera=self.calibration_camera, undistort=undistort)
    self.detections = [[] for _ in range(video.getVideoLength() +1)]
    video.play(self._detect)</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PeopleDetector.save_data"><code class="name flex">
<span>def <span class="ident">save_data</span></span>(<span>self, filepath)</span>
</code></dt>
<dd>
<section class="desc"><p>Save data extracted from video into file</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename to save data</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_data(self, filepath):
    # type: (str) -&gt; None
    &#34;&#34;&#34;Save data extracted from video into file

    Args:
        filepath (str): Filename to save data
    &#34;&#34;&#34;

    with open(filepath, &#39;wb&#39;) as f:
        pickle.dump([self.detections], f)
    return</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing"><code class="flex name class">
<span>class <span class="ident">PostProcessing</span></span>
<span>(</span><span>videofile, startframe, calibration)</span>
</code></dt>
<dd>
<section class="desc"><p>PostProcessing, Process data extracted from video</p>
<h2 id="params">Params</h2>
<dl>
<dt><strong><code>videofile</code></strong> :&ensp;<code>str</code></dt>
<dd>Video filename</dd>
<dt><strong><code>startframe</code></strong> :&ensp;<code>int</code></dt>
<dd>Set initial frame number</dd>
<dt><strong><code>calibration</code></strong> :&ensp;<code>str</code></dt>
<dd>Filepath of the camera calibration file</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class PostProcessing:
    &#34;&#34;&#34;PostProcessing, Process data extracted from video

      Params:
          videofile (str): Video filename
          startframe (int): Set initial frame number
          calibration (str): Filepath of the camera calibration file
      &#34;&#34;&#34;

    def __init__(self, videofile, startframe, calibration):
        self.video = videofile
        self.start_frame = startframe
        self.boxes_3d = None
        self.homography_data = None
        self.bird_view = None
        self.calibration_camera = None
        with open(calibration, &#39;rb&#39;) as f:
            self.calibration_camera = pickle.load(f)

    def load_data(self, filepath):
        # type: (str) -&gt; None
        &#34;&#34;&#34;Load data form file

        Args:
            filepath(str): filename contains data acquired by Detector
        &#34;&#34;&#34;

        with open(filepath, &#39;rb&#39;) as f:
            self.boxes_3d = pickle.load(f)[0]

        return None

    def save_data(self, filepath):
        # type: (str) -&gt; None
        &#34;&#34;&#34;Save data into file
        Data use this structure:
        frame1: [Box1, Box2, ...]
        frame2: [Box1, Box2, ...]
        ...
        frameN: [Box1, Box2, ...]


        Args:
            filepath (str): Filename to save data
        &#34;&#34;&#34;

        with open(filepath, &#39;wb&#39;) as f:
            pickle.dump([self.boxes_3d], f)

        return None

    def iterate_data(self):
        # type: () -&gt; (int, BoundBox)
        &#34;&#34;&#34;Generator to iterate data (BBox in each frame)

        Yields:
            int: Frame number
            BoundBox: Bounding Box
        &#34;&#34;&#34;

        for i in range(0, len(self.boxes_3d)):
            for box in self.boxes_3d[i]:
                yield i, box

    def get_video_and_homography(self, w, h, w2, h2, undistort=False):
        # type: (int, int, int, int, bool) -&gt; None
        &#34;&#34;&#34; Show tracking and homography simultaneously

        Args:
            w (int): tracking frame width
            h (int):  tracking Frame height
            w2 (int): homography frame width
            h2 (int): homography frame height
            undistort (bool): Apply rectification before processing frame
        &#34;&#34;&#34;

        video_tracking = StreamVideo(&#34;tracking&#34;, self.video, w, h, self.start_frame, calibration_camera=self.calibration_camera, undistort=undistort)
        video_homography = StreamVideo(&#34;homography&#34;, self.video, w2, h2, self.start_frame, calibration_camera=self.calibration_camera, undistort=undistort)

        retA, frameA = video_tracking.getNextFrame()
        retB, frameB = video_homography.getNextFrame()

        while retA:
            frameA = video_tracking.processSingleFrame(frameA, self._draw_boxes)
            frameB = video_homography.processSingleFrame(frameB, self._homography)

            cv2.imshow(&#34;tracking&#34;, frameA)
            cv2.waitKey(1)
            cv2.imshow(&#34;homography&#34;, frameB)
            cv2.waitKey(1)

            retA, frameA = video_tracking.getNextFrame()
            retB, frameB = video_homography.getNextFrame()

        return

    def get_homography(self, caption, w, h, background=False, save_video=None, undistort=False):
        # type: (str, int, int, bool, Optional[str], bool) -&gt; None
        &#34;&#34;&#34; Run homography video

        Args:
            caption (str): Window Name
            w (int): Frame width
            h (int): Frame height
            background (bool): Process in background mode
            save_video (str): Filename to save video
            undistort (bool): Apply rectification before processing frame
        &#34;&#34;&#34;

        video = StreamVideo(caption, self.video, w, h, self.start_frame, background, save_video, self.calibration_camera, undistort=undistort)
        video.play(self._homography)

    def compute_homography(self, pts_src, pts_dst, pic):
        # type: (np.ndarray, np.ndarray, str) -&gt; None
        &#34;&#34;&#34;Compute homography

        Args:
            pts_src (np.ndarray): Source points to compute homography
            pts_dst (np.ndarray): Destination points to compute homography
            pic (np.ndarray): Filename of a birdview picture
        &#34;&#34;&#34;

        pic = cv2.imread(pic, 0)
        self.bird_view = cv2.cvtColor(pic, cv2.COLOR_GRAY2BGR)

        pts_src = np.expand_dims(pts_src, 1).astype(np.float32)
        pts_dst = np.expand_dims(pts_dst, 1).astype(np.float32)
        hom = cv2.getPerspectiveTransform(pts_src, pts_dst)

        #pic2 = cv2.warpPerspective(pic, hom, (430, 813))
        #cv2.imshow(&#39;image&#39;, pic2)
        #cv2.waitKey(0)

        self.homography_data = copy.deepcopy(self.boxes_3d)
        iterator = ((i, j, box) for i in range(len(self.homography_data))
                    for j, box in enumerate(self.homography_data[i]))

        for i, j, box in iterator:
            x, y = box.rect.c_x, (box.rect.y + box.rect.h)
            points = np.array([[[x, y]]], dtype=&#39;float32&#39;)
            points = cv2.perspectiveTransform(points, hom)
            self.homography_data[i][j] = PointId(box.id, points[0][0][0], points[0][0][1], box.color)

    def _homography(self, frame, current_frame, _):
        # type: (np.ndarray, int, np.ndarray) -&gt; np.ndarray
        &#34;&#34;&#34; Processing frame function

        Args:
            frame (np.ndarray): Video frame
            current_frame (int): Number of the current frame

        Returns:
            np.ndarray: Frame with Points drawn on it
        &#34;&#34;&#34;

        bird_view = copy.deepcopy(self.bird_view)
        for point in self.homography_data[current_frame]:
            point.draw(bird_view)
        return bird_view

    def _smoot_get(self):
        # type: () -&gt; dict
        &#34;&#34;&#34;Auxiliary function to apply smoothing

        Returns:
            dict: Dict with BBox points on which to apply smooth
        &#34;&#34;&#34;

        h_points = {}
        iterator = self.iterate_data()

        for _, box in iterator:
            if box.id not in h_points.keys():
                h_points[box.id] = []
            h_points[box.id].append((box.rect.x, box.rect.y, box.rect.h, box.rect.w))
        return h_points

    def smooth(self, kernel_size):
        # type: (int) -&gt; None
        &#34;&#34;&#34;Smooth BBox

        Args:
            kernel_size (int): Number specifying kernel size (high more smoothing)

        &gt;&gt; NOTE: Smoothing is applied only to (y, h, w) BBox params
                 Smoothing on X coordinate is NOT visually good
        &#34;&#34;&#34;

        h_points = self._smoot_get()
        for key, val in h_points.items():
            h_points[key] = utils.smooth(np.array(val), kernel_size, kernel_dim=4)

        h_indexes = {}
        iterator = self.iterate_data()

        for _, box in iterator:
            if box.id not in h_indexes:
                h_indexes[box.id] = 0
            ix = h_indexes[box.id]
            box.rect.y = int(h_points[box.id][ix][1])
            box.rect.h = int(h_points[box.id][ix][2])
            box.rect.w = int(h_points[box.id][ix][3])
            h_indexes[box.id] += 1

    def _predict_z(self):
        # type: () -&gt; (float, float)
        &#34;&#34;&#34;Regression BBox points (y, depth)

        &gt;&gt;&gt; NOTE use y the bottom point of Bounding Box instead of Height

        Returns:
            (float, float): linear function coefficients (m, q)
        &#34;&#34;&#34;

        box_h, box_z = [], []
        iterator = self.iterate_data()

        for _, box in iterator:
            if box.rect.z is not None:
                box_h.append(box.rect.y + box.rect.h)
                box_z.append(box.rect.z)

        coeff = np.polyfit(np.array(box_h), np.array(box_z), 1)
        return coeff[0], coeff[1]

    def fill_z(self):
        # type: () -&gt; None
        &#34;&#34;&#34;Estimate missing depth information of BBox in data
           and fill them inplace

        &#34;&#34;&#34;

        m, q = self._predict_z()
        iterator = self.iterate_data()

        for _, box in iterator:
            if box.rect.z is None:
                box.rect.z = (box.rect.y + box.rect.h) * m + q

    def assign_id(self, mode=&#39;3d-camera&#39;, threshold=120, max_age=30, aruco_priority=True):
        # type: (str, int, int, bool) -&gt; None
        &#34;&#34;&#34;Assign BBox ids

        Args:
            mode (str): Distance mode: (3d, bottom, center)
            threshold (int): Tolerance threshold
            max_age (int): Max number of frames than remove BBox
            aruco_priority (bool): Assign aruco ID if available
        &#34;&#34;&#34;

        first_frame = 0
        box_assignment = BoxAssignment(threshold, max_age, self.calibration_camera)

        while len(self.boxes_3d[first_frame]) &lt; 1:
            first_frame += 1

        box_assignment.init(self.boxes_3d[first_frame])
        for i in range(first_frame, len(self.boxes_3d)):
            self.boxes_3d = box_assignment.assign(self.boxes_3d, i, mode, aruco_priority)

    def get_video(self, caption, w, h, background=False, save_video=None, undistort=False):
        # type: (str, int, int, bool, Optional[str], bool) -&gt; None
        &#34;&#34;&#34;Execute human detection

        Args:
            caption (str): Window Name
            w (int): Frame width
            h (int): Frame height
            background (bool): Process in background mode
            save_video (str): Filename to save video
            undistort (bool): Apply rectification before processing frame
        &#34;&#34;&#34;

        video = StreamVideo(caption, self.video, w, h, start_frame=self.start_frame, background=background, save_video=save_video, calibration_camera=self.calibration_camera, undistort=undistort)
        video.play(self._draw_boxes)

    def _draw_boxes(self, frame, frame_number, _):
        # type: (np.ndarray, int) -&gt; np.ndarray
        &#34;&#34;&#34;

        Args:
            frame (np.ndarray): Frame of video
            frame_number (int): Current frame number

        Returns:
            np.ndarray: Frame with BBox drawn on it
        &#34;&#34;&#34;

        for box in self.boxes_3d[frame_number]:
            frame = box.draw(frame)
        return frame

    def _prec(self, id, frame_num, window):
        # type: (int, int, int) -&gt; BoundBox or None
        &#34;&#34;&#34;Find previous BBox with id = id

        Args:
            id (int): BBox ID
            frame_num (int): Current frame number
            window (int): Temporal frames window to search
        &#34;&#34;&#34;

        window = frame_num - window
        while frame_num &gt; window and frame_num &gt; 0:
            iterator = (box for box in self.boxes_3d[frame_num] if (box.id == id) and (box.age == 0))
            for box in iterator:
                return box
            frame_num -= 1
        return None

    def _succ(self, id, frame_num, window):
        # type: (int, int, int) -&gt; BoundBox or None
        &#34;&#34;&#34;Find next BBox with id = id

        Args:
            id (int): BBox ID
            frame_num (int): Current frame number
            window (int): Temporal frames window to search
        &#34;&#34;&#34;

        window = frame_num + window
        while frame_num &lt; window and frame_num &lt; len(self.boxes_3d):
            iterator = (box for box in self.boxes_3d[frame_num] if (box.id == id) and (box.age == 0))
            for box in iterator:
                return box
            frame_num += 1
        return None

    def __get_ids_at_frame(self, frame_num):
        # type: (int) -&gt; list
        &#34;&#34;&#34;Get all BBox ids at frame = frame_num

        Args:
            frame_num (int): Frame number

        Returns:
            list: List of BBox ids
        &#34;&#34;&#34;

        box_in_frame = []
        for box in self.boxes_3d[frame_num]:
            box_in_frame.append(box.id)
        return box_in_frame

    def __get_all_id(self):
        # type: () -&gt; list
        &#34;&#34;&#34;Get all uniques BBox ids in data

        Returns:
              list: List of ids
        &#34;&#34;&#34;

        box_ids = []
        iterator = self.iterate_data()
        iterator = ((_, box) for _, box in iterator if box.id not in box_ids)

        for _, box in iterator:
            box_ids.append(box.id)
        return box_ids

    def fill(self, window=90):
        # type: (int) -&gt; None
        &#34;&#34;&#34;Fill missing detections interpolating data

        Args:
            window (int): Temporal window size
        &#34;&#34;&#34;

        all_ids = self.__get_all_id()
        iterator = ((i, _id) for _id in all_ids for i in range(0, len(self.boxes_3d))
                    if _id not in self.__get_ids_at_frame(i))

        for i, _id in iterator:
            prec_box, next_box = self._prec(_id, i, window), self._succ(_id, i, window)
            if prec_box is not None and next_box is not None:
                prec_box.rect = prec_box.rect.interpolate(next_box.rect)
                self.boxes_3d[i].append(prec_box)
        return None

    def plot_3d(self, box_id, savepath=None):
        # type: (int, str) -&gt; None
        &#34;&#34;&#34;Plot 3d path of a BBox with id = box_id

        Args:
            box_id (int): ID BBox to plot
            savepath (str): Filename to save video
        &#34;&#34;&#34;

        points = []
        out = None
        iterator = self.iterate_data()

        if savepath is not None:
            out = cv2.VideoWriter(savepath, cv2.VideoWriter_fourcc(&#39;M&#39;, &#39;J&#39;, &#39;P&#39;, &#39;G&#39;), 30, (800, 800))

        for i, box in iterator:
            if box.id == box_id:
                points.append((box.rect.x, box.rect.y+box.rect.h, box.rect.z))
            else:
                points.append((None, None, None))

        ts = np.array(points, dtype=np.float32)
        for i in range(1, len(ts)):
            fig = utils.show_temporal_sequence(ts[:i], center=np.nanmean(ts, 0), radius=np.sqrt(np.nanvar(ts)),
                                               colormap=&#39;jet&#39;, swapaxis=False)
            fig.savefig(&#34;frame.png&#34;)
            img = cv2.imread(&#34;frame.png&#34;)

            if out:
                out.write(img)
            else:
                cv2.imshow(&#34;Plot 3d&#34;, img)
                cv2.waitKey(1)
        if out:
            out.release()
        return</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tetratrack.videoprocessing.PostProcessing.assign_id"><code class="name flex">
<span>def <span class="ident">assign_id</span></span>(<span>self, mode='3d-camera', threshold=120, max_age=30, aruco_priority=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Assign BBox ids</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>Distance mode: (3d, bottom, center)</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>int</code></dt>
<dd>Tolerance threshold</dd>
<dt><strong><code>max_age</code></strong> :&ensp;<code>int</code></dt>
<dd>Max number of frames than remove BBox</dd>
<dt><strong><code>aruco_priority</code></strong> :&ensp;<code>bool</code></dt>
<dd>Assign aruco ID if available</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def assign_id(self, mode=&#39;3d-camera&#39;, threshold=120, max_age=30, aruco_priority=True):
    # type: (str, int, int, bool) -&gt; None
    &#34;&#34;&#34;Assign BBox ids

    Args:
        mode (str): Distance mode: (3d, bottom, center)
        threshold (int): Tolerance threshold
        max_age (int): Max number of frames than remove BBox
        aruco_priority (bool): Assign aruco ID if available
    &#34;&#34;&#34;

    first_frame = 0
    box_assignment = BoxAssignment(threshold, max_age, self.calibration_camera)

    while len(self.boxes_3d[first_frame]) &lt; 1:
        first_frame += 1

    box_assignment.init(self.boxes_3d[first_frame])
    for i in range(first_frame, len(self.boxes_3d)):
        self.boxes_3d = box_assignment.assign(self.boxes_3d, i, mode, aruco_priority)</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.compute_homography"><code class="name flex">
<span>def <span class="ident">compute_homography</span></span>(<span>self, pts_src, pts_dst, pic)</span>
</code></dt>
<dd>
<section class="desc"><p>Compute homography</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pts_src</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Source points to compute homography</dd>
<dt><strong><code>pts_dst</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Destination points to compute homography</dd>
<dt><strong><code>pic</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Filename of a birdview picture</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def compute_homography(self, pts_src, pts_dst, pic):
    # type: (np.ndarray, np.ndarray, str) -&gt; None
    &#34;&#34;&#34;Compute homography

    Args:
        pts_src (np.ndarray): Source points to compute homography
        pts_dst (np.ndarray): Destination points to compute homography
        pic (np.ndarray): Filename of a birdview picture
    &#34;&#34;&#34;

    pic = cv2.imread(pic, 0)
    self.bird_view = cv2.cvtColor(pic, cv2.COLOR_GRAY2BGR)

    pts_src = np.expand_dims(pts_src, 1).astype(np.float32)
    pts_dst = np.expand_dims(pts_dst, 1).astype(np.float32)
    hom = cv2.getPerspectiveTransform(pts_src, pts_dst)

    #pic2 = cv2.warpPerspective(pic, hom, (430, 813))
    #cv2.imshow(&#39;image&#39;, pic2)
    #cv2.waitKey(0)

    self.homography_data = copy.deepcopy(self.boxes_3d)
    iterator = ((i, j, box) for i in range(len(self.homography_data))
                for j, box in enumerate(self.homography_data[i]))

    for i, j, box in iterator:
        x, y = box.rect.c_x, (box.rect.y + box.rect.h)
        points = np.array([[[x, y]]], dtype=&#39;float32&#39;)
        points = cv2.perspectiveTransform(points, hom)
        self.homography_data[i][j] = PointId(box.id, points[0][0][0], points[0][0][1], box.color)</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.fill"><code class="name flex">
<span>def <span class="ident">fill</span></span>(<span>self, window=90)</span>
</code></dt>
<dd>
<section class="desc"><p>Fill missing detections interpolating data</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>window</code></strong> :&ensp;<code>int</code></dt>
<dd>Temporal window size</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fill(self, window=90):
    # type: (int) -&gt; None
    &#34;&#34;&#34;Fill missing detections interpolating data

    Args:
        window (int): Temporal window size
    &#34;&#34;&#34;

    all_ids = self.__get_all_id()
    iterator = ((i, _id) for _id in all_ids for i in range(0, len(self.boxes_3d))
                if _id not in self.__get_ids_at_frame(i))

    for i, _id in iterator:
        prec_box, next_box = self._prec(_id, i, window), self._succ(_id, i, window)
        if prec_box is not None and next_box is not None:
            prec_box.rect = prec_box.rect.interpolate(next_box.rect)
            self.boxes_3d[i].append(prec_box)
    return None</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.fill_z"><code class="name flex">
<span>def <span class="ident">fill_z</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Estimate missing depth information of BBox in data
and fill them inplace</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fill_z(self):
    # type: () -&gt; None
    &#34;&#34;&#34;Estimate missing depth information of BBox in data
       and fill them inplace

    &#34;&#34;&#34;

    m, q = self._predict_z()
    iterator = self.iterate_data()

    for _, box in iterator:
        if box.rect.z is None:
            box.rect.z = (box.rect.y + box.rect.h) * m + q</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.get_homography"><code class="name flex">
<span>def <span class="ident">get_homography</span></span>(<span>self, caption, w, h, background=False, save_video=None, undistort=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Run homography video</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>caption</code></strong> :&ensp;<code>str</code></dt>
<dd>Window Name</dd>
<dt><strong><code>w</code></strong> :&ensp;<code>int</code></dt>
<dd>Frame width</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>int</code></dt>
<dd>Frame height</dd>
<dt><strong><code>background</code></strong> :&ensp;<code>bool</code></dt>
<dd>Process in background mode</dd>
<dt><strong><code>save_video</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename to save video</dd>
<dt><strong><code>undistort</code></strong> :&ensp;<code>bool</code></dt>
<dd>Apply rectification before processing frame</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_homography(self, caption, w, h, background=False, save_video=None, undistort=False):
    # type: (str, int, int, bool, Optional[str], bool) -&gt; None
    &#34;&#34;&#34; Run homography video

    Args:
        caption (str): Window Name
        w (int): Frame width
        h (int): Frame height
        background (bool): Process in background mode
        save_video (str): Filename to save video
        undistort (bool): Apply rectification before processing frame
    &#34;&#34;&#34;

    video = StreamVideo(caption, self.video, w, h, self.start_frame, background, save_video, self.calibration_camera, undistort=undistort)
    video.play(self._homography)</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.get_video"><code class="name flex">
<span>def <span class="ident">get_video</span></span>(<span>self, caption, w, h, background=False, save_video=None, undistort=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Execute human detection</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>caption</code></strong> :&ensp;<code>str</code></dt>
<dd>Window Name</dd>
<dt><strong><code>w</code></strong> :&ensp;<code>int</code></dt>
<dd>Frame width</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>int</code></dt>
<dd>Frame height</dd>
<dt><strong><code>background</code></strong> :&ensp;<code>bool</code></dt>
<dd>Process in background mode</dd>
<dt><strong><code>save_video</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename to save video</dd>
<dt><strong><code>undistort</code></strong> :&ensp;<code>bool</code></dt>
<dd>Apply rectification before processing frame</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_video(self, caption, w, h, background=False, save_video=None, undistort=False):
    # type: (str, int, int, bool, Optional[str], bool) -&gt; None
    &#34;&#34;&#34;Execute human detection

    Args:
        caption (str): Window Name
        w (int): Frame width
        h (int): Frame height
        background (bool): Process in background mode
        save_video (str): Filename to save video
        undistort (bool): Apply rectification before processing frame
    &#34;&#34;&#34;

    video = StreamVideo(caption, self.video, w, h, start_frame=self.start_frame, background=background, save_video=save_video, calibration_camera=self.calibration_camera, undistort=undistort)
    video.play(self._draw_boxes)</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.get_video_and_homography"><code class="name flex">
<span>def <span class="ident">get_video_and_homography</span></span>(<span>self, w, h, w2, h2, undistort=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Show tracking and homography simultaneously</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>w</code></strong> :&ensp;<code>int</code></dt>
<dd>tracking frame width</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>int</code></dt>
<dd>tracking Frame height</dd>
<dt><strong><code>w2</code></strong> :&ensp;<code>int</code></dt>
<dd>homography frame width</dd>
<dt><strong><code>h2</code></strong> :&ensp;<code>int</code></dt>
<dd>homography frame height</dd>
<dt><strong><code>undistort</code></strong> :&ensp;<code>bool</code></dt>
<dd>Apply rectification before processing frame</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_video_and_homography(self, w, h, w2, h2, undistort=False):
    # type: (int, int, int, int, bool) -&gt; None
    &#34;&#34;&#34; Show tracking and homography simultaneously

    Args:
        w (int): tracking frame width
        h (int):  tracking Frame height
        w2 (int): homography frame width
        h2 (int): homography frame height
        undistort (bool): Apply rectification before processing frame
    &#34;&#34;&#34;

    video_tracking = StreamVideo(&#34;tracking&#34;, self.video, w, h, self.start_frame, calibration_camera=self.calibration_camera, undistort=undistort)
    video_homography = StreamVideo(&#34;homography&#34;, self.video, w2, h2, self.start_frame, calibration_camera=self.calibration_camera, undistort=undistort)

    retA, frameA = video_tracking.getNextFrame()
    retB, frameB = video_homography.getNextFrame()

    while retA:
        frameA = video_tracking.processSingleFrame(frameA, self._draw_boxes)
        frameB = video_homography.processSingleFrame(frameB, self._homography)

        cv2.imshow(&#34;tracking&#34;, frameA)
        cv2.waitKey(1)
        cv2.imshow(&#34;homography&#34;, frameB)
        cv2.waitKey(1)

        retA, frameA = video_tracking.getNextFrame()
        retB, frameB = video_homography.getNextFrame()

    return</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.iterate_data"><code class="name flex">
<span>def <span class="ident">iterate_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Generator to iterate data (BBox in each frame)</p>
<h2 id="yields">Yields</h2>
<dl>
<dt><strong><code>int</code></strong></dt>
<dd>Frame number</dd>
<dt><strong><code>BoundBox</code></strong></dt>
<dd>Bounding Box</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def iterate_data(self):
    # type: () -&gt; (int, BoundBox)
    &#34;&#34;&#34;Generator to iterate data (BBox in each frame)

    Yields:
        int: Frame number
        BoundBox: Bounding Box
    &#34;&#34;&#34;

    for i in range(0, len(self.boxes_3d)):
        for box in self.boxes_3d[i]:
            yield i, box</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.load_data"><code class="name flex">
<span>def <span class="ident">load_data</span></span>(<span>self, filepath)</span>
</code></dt>
<dd>
<section class="desc"><p>Load data form file</p>
<h2 id="args">Args</h2>
<p>filepath(str): filename contains data acquired by Detector</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_data(self, filepath):
    # type: (str) -&gt; None
    &#34;&#34;&#34;Load data form file

    Args:
        filepath(str): filename contains data acquired by Detector
    &#34;&#34;&#34;

    with open(filepath, &#39;rb&#39;) as f:
        self.boxes_3d = pickle.load(f)[0]

    return None</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.plot_3d"><code class="name flex">
<span>def <span class="ident">plot_3d</span></span>(<span>self, box_id, savepath=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Plot 3d path of a BBox with id = box_id</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>box_id</code></strong> :&ensp;<code>int</code></dt>
<dd>ID BBox to plot</dd>
<dt><strong><code>savepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename to save video</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plot_3d(self, box_id, savepath=None):
    # type: (int, str) -&gt; None
    &#34;&#34;&#34;Plot 3d path of a BBox with id = box_id

    Args:
        box_id (int): ID BBox to plot
        savepath (str): Filename to save video
    &#34;&#34;&#34;

    points = []
    out = None
    iterator = self.iterate_data()

    if savepath is not None:
        out = cv2.VideoWriter(savepath, cv2.VideoWriter_fourcc(&#39;M&#39;, &#39;J&#39;, &#39;P&#39;, &#39;G&#39;), 30, (800, 800))

    for i, box in iterator:
        if box.id == box_id:
            points.append((box.rect.x, box.rect.y+box.rect.h, box.rect.z))
        else:
            points.append((None, None, None))

    ts = np.array(points, dtype=np.float32)
    for i in range(1, len(ts)):
        fig = utils.show_temporal_sequence(ts[:i], center=np.nanmean(ts, 0), radius=np.sqrt(np.nanvar(ts)),
                                           colormap=&#39;jet&#39;, swapaxis=False)
        fig.savefig(&#34;frame.png&#34;)
        img = cv2.imread(&#34;frame.png&#34;)

        if out:
            out.write(img)
        else:
            cv2.imshow(&#34;Plot 3d&#34;, img)
            cv2.waitKey(1)
    if out:
        out.release()
    return</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.save_data"><code class="name flex">
<span>def <span class="ident">save_data</span></span>(<span>self, filepath)</span>
</code></dt>
<dd>
<section class="desc"><p>Save data into file
Data use this structure:
frame1: [Box1, Box2, &hellip;]
frame2: [Box1, Box2, &hellip;]
&hellip;
frameN: [Box1, Box2, &hellip;]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename to save data</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_data(self, filepath):
    # type: (str) -&gt; None
    &#34;&#34;&#34;Save data into file
    Data use this structure:
    frame1: [Box1, Box2, ...]
    frame2: [Box1, Box2, ...]
    ...
    frameN: [Box1, Box2, ...]


    Args:
        filepath (str): Filename to save data
    &#34;&#34;&#34;

    with open(filepath, &#39;wb&#39;) as f:
        pickle.dump([self.boxes_3d], f)

    return None</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.PostProcessing.smooth"><code class="name flex">
<span>def <span class="ident">smooth</span></span>(<span>self, kernel_size)</span>
</code></dt>
<dd>
<section class="desc"><p>Smooth BBox</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number specifying kernel size (high more smoothing)</dd>
</dl>
<blockquote>
<blockquote>
<p>NOTE: Smoothing is applied only to (y, h, w) BBox params
Smoothing on X coordinate is NOT visually good</p>
</blockquote>
</blockquote></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def smooth(self, kernel_size):
    # type: (int) -&gt; None
    &#34;&#34;&#34;Smooth BBox

    Args:
        kernel_size (int): Number specifying kernel size (high more smoothing)

    &gt;&gt; NOTE: Smoothing is applied only to (y, h, w) BBox params
             Smoothing on X coordinate is NOT visually good
    &#34;&#34;&#34;

    h_points = self._smoot_get()
    for key, val in h_points.items():
        h_points[key] = utils.smooth(np.array(val), kernel_size, kernel_dim=4)

    h_indexes = {}
    iterator = self.iterate_data()

    for _, box in iterator:
        if box.id not in h_indexes:
            h_indexes[box.id] = 0
        ix = h_indexes[box.id]
        box.rect.y = int(h_points[box.id][ix][1])
        box.rect.h = int(h_points[box.id][ix][2])
        box.rect.w = int(h_points[box.id][ix][3])
        h_indexes[box.id] += 1</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="tetratrack.videoprocessing.StreamVideo"><code class="flex name class">
<span>class <span class="ident">StreamVideo</span></span>
<span>(</span><span>caption, video, w, h, start_frame=0, fps=0, pbar=0, background=True, save_video=None, calibration_camera=None, undistort=False)</span>
</code></dt>
<dd>
<section class="desc"><p>StreamVideo, Class to play video frame-by-frame</p>
<h2 id="params">Params</h2>
<dl>
<dt><strong><code>caption</code></strong> :&ensp;<code>str</code></dt>
<dd>Window caption</dd>
<dt><strong><code>video</code></strong> :&ensp;<code>str</code></dt>
<dd>Video filename</dd>
<dt><strong><code>w</code></strong> :&ensp;<code>int</code></dt>
<dd>Width</dd>
<dt><strong><code>h</code></strong> :&ensp;<code>int</code></dt>
<dd>Height</dd>
<dt><strong><code>start_frame</code></strong> :&ensp;<code>int</code></dt>
<dd>Start video from this frame number</dd>
<dt><strong><code>fps</code></strong> :&ensp;<code>int</code></dt>
<dd>Set video framerate (0: realtime)</dd>
<dt><strong><code>pbar</code></strong> :&ensp;<code>int</code></dt>
<dd>Progress bar id</dd>
<dt><strong><code>background</code></strong> :&ensp;<code>bool</code></dt>
<dd>Processing in background</dd>
<dt><strong><code>save_video</code></strong> :&ensp;<code>str</code></dt>
<dd>Filename to save video</dd>
<dt><strong><code>calibration_camera</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Matrix calibration camera</dd>
<dt><strong><code>undistort</code></strong> :&ensp;<code>bool</code></dt>
<dd>Apply frame rectification</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class StreamVideo:
    &#34;&#34;&#34;StreamVideo, Class to play video frame-by-frame

    Params:
        caption (str): Window caption
        video (str): Video filename
        w (int): Width
        h (int): Height
        start_frame (int): Start video from this frame number
        fps (int): Set video framerate (0: realtime)
        pbar (int): Progress bar id
        background (bool): Processing in background
        save_video (str): Filename to save video
        calibration_camera (np.ndarray): Matrix calibration camera
        undistort (bool): Apply frame rectification
    &#34;&#34;&#34;

    def __init__(self, caption, video, w, h, start_frame=0, fps=0, pbar=0, background=True, save_video=None, calibration_camera=None, undistort=False):
        self.h, self.w = h, w
        self.frame_number = 0
        self.background = background
        self.save_video = save_video
        self.caption = caption
        self.start_frame = start_frame
        self.calibration_camera = calibration_camera
        self.video = video
        self.undistort = undistort
        self.pbar = pbar
        self.videoCap = cv2.VideoCapture(self.video)
        self.videoCap.set(1, self.start_frame)
        self.fsampling = round(self.getFps() / fps) if fps != 0 else 1

        if self.save_video is not None:
            self.vid_writer1 = cv2.VideoWriter(self.save_video, cv2.VideoWriter_fourcc(&#39;M&#39;, &#39;J&#39;, &#39;P&#39;, &#39;G&#39;), 30,
                                          (self.w, self.h))

    def getFps(self):
        # type: () -&gt; int
        &#34;&#34;&#34;Get video-file frames per second.

        Returns:
            int: Fps
        &#34;&#34;&#34;

        return int(self.videoCap.get(cv2.CAP_PROP_FPS))

    def getVideoLength(self):
        # type: () -&gt; int
        &#34;&#34;&#34;Get total number of frames in video-file.

        Returns:
            int: Number of frames
        &#34;&#34;&#34;

        return int(self.videoCap.get(cv2.CAP_PROP_FRAME_COUNT))

    def getNextFrame(self):
        # type: () -&gt; (int, np.ndarray)
        &#34;&#34;&#34;Read next frame from video stream

        Returns:
            int: Return value
            np.ndarray: Frame
        &#34;&#34;&#34;

        ret, img = None, None
        fsample = self.fsampling

        # if no video opened return None
        if self.videoCap.isOpened():
            while fsample &gt; 0:
                ret, img = self.videoCap.read()
                self.frame_number += 1
                fsample -=1
        else:
            print(&#34;{} Error VideoCap not opened, quitting ... &#34;.format(self.video))

        return ret, img

    def processSingleFrame(self, frame, func):
        # type: (np.ndarray, classmethod) -&gt; np.ndarray
        &#34;&#34;&#34;Processing single frame

        Args:
            frame (np.ndarray): Video frame
            func (classmethod): Function to process frame

        Returns:
             np.ndarray: Frame processed
        &#34;&#34;&#34;

        # Resize frame
        frame = cv2.resize(frame, (self.w, self.h), interpolation=cv2.INTER_AREA)

        # Undistort
        if self.undistort == True:
            _, mtx, dist, _, _, newMtx = self.calibration_camera
            self.undistort = cv2.initUndistortRectifyMap(mtx, dist, None, newMtx,
                                                             (self.w, self.h), cv2.CV_32FC1)

        if isinstance(self.undistort, tuple):
            frame = cv2.remap(frame, self.undistort[0], self.undistort[1],
                                   interpolation=cv2.INTER_LINEAR, borderValue=0)


        # Call function passed as argument
        frame = func(frame, self.frame_number, self.background)

        if self.save_video is not None:
            self.vid_writer1.write(frame)

        return frame


    def play(self, func):
        # type: (classmethod) -&gt; None
        &#34;&#34;&#34;Play video frame by frame and apply a function `func`

        Args:
            func (classmethod): Generic function to execute at each frame
        &#34;&#34;&#34;

        print(&#34;Loading video...&#34;)
        ret, frame = self.getNextFrame()

        with tqdm(desc=&#39;Processing: &#39; + self.caption, position=self.pbar, unit=&#39;it&#39;, total=self.getVideoLength()) as pbar:
            while ret:
                frame = self.processSingleFrame(frame, func)
                if not self.background:
                    cv2.imshow(self.caption, frame)
                    cv2.waitKey(1)

                ret, frame = self.getNextFrame()
                pbar.update(self.fsampling)

        if self.save_video is not None:
            self.vid_writer1.release()

        print(self.caption, &#34; done!&#34;)
        return</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tetratrack.videoprocessing.StreamVideo.getFps"><code class="name flex">
<span>def <span class="ident">getFps</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get video-file frames per second.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>int</code></strong></dt>
<dd>Fps</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def getFps(self):
    # type: () -&gt; int
    &#34;&#34;&#34;Get video-file frames per second.

    Returns:
        int: Fps
    &#34;&#34;&#34;

    return int(self.videoCap.get(cv2.CAP_PROP_FPS))</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.StreamVideo.getNextFrame"><code class="name flex">
<span>def <span class="ident">getNextFrame</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Read next frame from video stream</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>int</code></strong></dt>
<dd>Return value</dd>
<dt><code>np.ndarray</code>: <code>Frame</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def getNextFrame(self):
    # type: () -&gt; (int, np.ndarray)
    &#34;&#34;&#34;Read next frame from video stream

    Returns:
        int: Return value
        np.ndarray: Frame
    &#34;&#34;&#34;

    ret, img = None, None
    fsample = self.fsampling

    # if no video opened return None
    if self.videoCap.isOpened():
        while fsample &gt; 0:
            ret, img = self.videoCap.read()
            self.frame_number += 1
            fsample -=1
    else:
        print(&#34;{} Error VideoCap not opened, quitting ... &#34;.format(self.video))

    return ret, img</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.StreamVideo.getVideoLength"><code class="name flex">
<span>def <span class="ident">getVideoLength</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get total number of frames in video-file.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>int</code></strong></dt>
<dd>Number of frames</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def getVideoLength(self):
    # type: () -&gt; int
    &#34;&#34;&#34;Get total number of frames in video-file.

    Returns:
        int: Number of frames
    &#34;&#34;&#34;

    return int(self.videoCap.get(cv2.CAP_PROP_FRAME_COUNT))</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.StreamVideo.play"><code class="name flex">
<span>def <span class="ident">play</span></span>(<span>self, func)</span>
</code></dt>
<dd>
<section class="desc"><p>Play video frame by frame and apply a function <code>func</code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>func</code></strong> :&ensp;<code>classmethod</code></dt>
<dd>Generic function to execute at each frame</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def play(self, func):
    # type: (classmethod) -&gt; None
    &#34;&#34;&#34;Play video frame by frame and apply a function `func`

    Args:
        func (classmethod): Generic function to execute at each frame
    &#34;&#34;&#34;

    print(&#34;Loading video...&#34;)
    ret, frame = self.getNextFrame()

    with tqdm(desc=&#39;Processing: &#39; + self.caption, position=self.pbar, unit=&#39;it&#39;, total=self.getVideoLength()) as pbar:
        while ret:
            frame = self.processSingleFrame(frame, func)
            if not self.background:
                cv2.imshow(self.caption, frame)
                cv2.waitKey(1)

            ret, frame = self.getNextFrame()
            pbar.update(self.fsampling)

    if self.save_video is not None:
        self.vid_writer1.release()

    print(self.caption, &#34; done!&#34;)
    return</code></pre>
</details>
</dd>
<dt id="tetratrack.videoprocessing.StreamVideo.processSingleFrame"><code class="name flex">
<span>def <span class="ident">processSingleFrame</span></span>(<span>self, frame, func)</span>
</code></dt>
<dd>
<section class="desc"><p>Processing single frame</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>frame</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Video frame</dd>
<dt><strong><code>func</code></strong> :&ensp;<code>classmethod</code></dt>
<dd>Function to process frame</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code>: <code>Frame</code> <code>processed</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def processSingleFrame(self, frame, func):
    # type: (np.ndarray, classmethod) -&gt; np.ndarray
    &#34;&#34;&#34;Processing single frame

    Args:
        frame (np.ndarray): Video frame
        func (classmethod): Function to process frame

    Returns:
         np.ndarray: Frame processed
    &#34;&#34;&#34;

    # Resize frame
    frame = cv2.resize(frame, (self.w, self.h), interpolation=cv2.INTER_AREA)

    # Undistort
    if self.undistort == True:
        _, mtx, dist, _, _, newMtx = self.calibration_camera
        self.undistort = cv2.initUndistortRectifyMap(mtx, dist, None, newMtx,
                                                         (self.w, self.h), cv2.CV_32FC1)

    if isinstance(self.undistort, tuple):
        frame = cv2.remap(frame, self.undistort[0], self.undistort[1],
                               interpolation=cv2.INTER_LINEAR, borderValue=0)


    # Call function passed as argument
    frame = func(frame, self.frame_number, self.background)

    if self.save_video is not None:
        self.vid_writer1.write(frame)

    return frame</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tetratrack" href="index.html">tetratrack</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tetratrack.videoprocessing.BoxAssignment" href="#tetratrack.videoprocessing.BoxAssignment">BoxAssignment</a></code></h4>
<ul class="">
<li><code><a title="tetratrack.videoprocessing.BoxAssignment.assign" href="#tetratrack.videoprocessing.BoxAssignment.assign">assign</a></code></li>
<li><code><a title="tetratrack.videoprocessing.BoxAssignment.assign_aruco" href="#tetratrack.videoprocessing.BoxAssignment.assign_aruco">assign_aruco</a></code></li>
<li><code><a title="tetratrack.videoprocessing.BoxAssignment.init" href="#tetratrack.videoprocessing.BoxAssignment.init">init</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="tetratrack.videoprocessing.PeopleDetector" href="#tetratrack.videoprocessing.PeopleDetector">PeopleDetector</a></code></h4>
<ul class="">
<li><code><a title="tetratrack.videoprocessing.PeopleDetector.callback" href="#tetratrack.videoprocessing.PeopleDetector.callback">callback</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PeopleDetector.run" href="#tetratrack.videoprocessing.PeopleDetector.run">run</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PeopleDetector.save_data" href="#tetratrack.videoprocessing.PeopleDetector.save_data">save_data</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="tetratrack.videoprocessing.PostProcessing" href="#tetratrack.videoprocessing.PostProcessing">PostProcessing</a></code></h4>
<ul class="">
<li><code><a title="tetratrack.videoprocessing.PostProcessing.assign_id" href="#tetratrack.videoprocessing.PostProcessing.assign_id">assign_id</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.compute_homography" href="#tetratrack.videoprocessing.PostProcessing.compute_homography">compute_homography</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.fill" href="#tetratrack.videoprocessing.PostProcessing.fill">fill</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.fill_z" href="#tetratrack.videoprocessing.PostProcessing.fill_z">fill_z</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.get_homography" href="#tetratrack.videoprocessing.PostProcessing.get_homography">get_homography</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.get_video" href="#tetratrack.videoprocessing.PostProcessing.get_video">get_video</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.get_video_and_homography" href="#tetratrack.videoprocessing.PostProcessing.get_video_and_homography">get_video_and_homography</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.iterate_data" href="#tetratrack.videoprocessing.PostProcessing.iterate_data">iterate_data</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.load_data" href="#tetratrack.videoprocessing.PostProcessing.load_data">load_data</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.plot_3d" href="#tetratrack.videoprocessing.PostProcessing.plot_3d">plot_3d</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.save_data" href="#tetratrack.videoprocessing.PostProcessing.save_data">save_data</a></code></li>
<li><code><a title="tetratrack.videoprocessing.PostProcessing.smooth" href="#tetratrack.videoprocessing.PostProcessing.smooth">smooth</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="tetratrack.videoprocessing.StreamVideo" href="#tetratrack.videoprocessing.StreamVideo">StreamVideo</a></code></h4>
<ul class="">
<li><code><a title="tetratrack.videoprocessing.StreamVideo.getFps" href="#tetratrack.videoprocessing.StreamVideo.getFps">getFps</a></code></li>
<li><code><a title="tetratrack.videoprocessing.StreamVideo.getNextFrame" href="#tetratrack.videoprocessing.StreamVideo.getNextFrame">getNextFrame</a></code></li>
<li><code><a title="tetratrack.videoprocessing.StreamVideo.getVideoLength" href="#tetratrack.videoprocessing.StreamVideo.getVideoLength">getVideoLength</a></code></li>
<li><code><a title="tetratrack.videoprocessing.StreamVideo.play" href="#tetratrack.videoprocessing.StreamVideo.play">play</a></code></li>
<li><code><a title="tetratrack.videoprocessing.StreamVideo.processSingleFrame" href="#tetratrack.videoprocessing.StreamVideo.processSingleFrame">processSingleFrame</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>